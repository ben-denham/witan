{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary-class Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from witan_experiments import (is_cached,\n",
    "                               save_to_cache,\n",
    "                               load_from_cache,\n",
    "                               run_experiments)\n",
    "from witan_experiments.evaluation import (summarise_experiments,\n",
    "                                          metric_line_grid,\n",
    "                                          build_metric_df,\n",
    "                                          display_metric_table,\n",
    "                                          display_friedman_test,\n",
    "                                          median_stds_df)\n",
    "from witan_experiments.config import prepare_experiment_configs\n",
    "from witan_experiments.rule_seeders import BlankRS, AccRS\n",
    "from witan_experiments.rule_generators import (TrueRG,\n",
    "                                               IWSBinaryRG,\n",
    "                                               WitanRG,\n",
    "                                               SnubaRG,\n",
    "                                               SemiSupervisedRG,\n",
    "                                               ActiveLearningRG,\n",
    "                                               RandomLabellingRG,\n",
    "                                               CbiRG,\n",
    "                                               HdcRG)\n",
    "from witan_experiments.labellers import SnorkelLblr\n",
    "from witan_experiments.models import AnnClf\n",
    "from witan_experiments.utils import inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset_generators = {\n",
    "    'Full supervision': TrueRG(),\n",
    "    'Wɪᴛᴀɴ': WitanRG(),\n",
    "    'Wɪᴛᴀɴ-Core': WitanRG(a=False, o=1),\n",
    "    'IWS-AS': IWSBinaryRG(acq='AS'),\n",
    "    'IWS-LSE-AC': IWSBinaryRG(acq='LSE'),\n",
    "    'Snuba': SnubaRG(),\n",
    "    'HDC': HdcRG(),\n",
    "    'CBI': CbiRG(clf=AnnClf()),\n",
    "    'Semi-supervised': SemiSupervisedRG(),\n",
    "    'Active learning': ActiveLearningRG(clf=AnnClf(), init_count=0),\n",
    "    'Random labelling': RandomLabellingRG(),\n",
    "}\n",
    "\n",
    "base_config = dict(\n",
    "    rule_seeder=[BlankRS()],\n",
    "    rngseed=[1, 2, 3, 4, 5],\n",
    "    labeller=[SnorkelLblr()],\n",
    "    classifier=[AnnClf()],\n",
    ")\n",
    "\n",
    "parallel_configs = [\n",
    "    # Unseeded ruleset_generators that are not affected by rngseed\n",
    "    # (so we only need to execute them for the first rngseed)\n",
    "    {\n",
    "        **base_config,\n",
    "        **dict(\n",
    "            rngseed=base_config['rngseed'][:1],\n",
    "            ruleset_generator=[\n",
    "                ruleset_generators['Full supervision'],\n",
    "                ruleset_generators['Wɪᴛᴀɴ'],\n",
    "                ruleset_generators['Wɪᴛᴀɴ-Core'],\n",
    "                ruleset_generators['HDC'],\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "    # Unseeded ruleset_generators\n",
    "    {\n",
    "        **base_config,\n",
    "        **dict(\n",
    "            ruleset_generator=[\n",
    "                ruleset_generators['Snuba'],\n",
    "                ruleset_generators['Semi-supervised'],\n",
    "                ruleset_generators['Active learning'],\n",
    "                ruleset_generators['Random labelling'],\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "    # Seeded ruleset_generators\n",
    "    {\n",
    "        **base_config,\n",
    "        **dict(\n",
    "            rule_seeder=[AccRS()],\n",
    "            ruleset_generator=[\n",
    "                ruleset_generators['Wɪᴛᴀɴ'],\n",
    "                ruleset_generators['Wɪᴛᴀɴ-Core'],\n",
    "                ruleset_generators['CBI'],\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "serial_configs = [\n",
    "    # Unseeded ruleset_generators\n",
    "    {\n",
    "        **base_config,\n",
    "        **dict(\n",
    "            ruleset_generator=[\n",
    "                ruleset_generators['IWS-AS'],\n",
    "                ruleset_generators['IWS-LSE-AC'],\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "    # Seeded ruleset_generators\n",
    "    {\n",
    "        **base_config,\n",
    "        **dict(\n",
    "            rule_seeder=[AccRS()],\n",
    "            ruleset_generator=[\n",
    "                ruleset_generators['IWS-AS'],\n",
    "                ruleset_generators['IWS-LSE-AC'],\n",
    "            ],\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "full_ic = [10, 25, 50, 100, 150, 200]\n",
    "min_ic = [25, 100]\n",
    "dataset_to_ic = {\n",
    "    'imdb': full_ic,\n",
    "    'imdb_genre': min_ic,\n",
    "    'bias_pa': full_ic,\n",
    "    'bias_pt': min_ic,\n",
    "    'bias_jp': min_ic,\n",
    "    'bias_pp': min_ic,\n",
    "    'amazon': min_ic,\n",
    "    'yelp': min_ic,\n",
    "    'plots': min_ic,\n",
    "    'fakenews': full_ic,\n",
    "    'binary_dbpedia': min_ic,\n",
    "    'binary_agnews': full_ic,\n",
    "    'airline_tweets': min_ic,\n",
    "    'damage': min_ic,\n",
    "    'spam': min_ic,\n",
    "}\n",
    "\n",
    "dataset_configs = {\n",
    "    dataset: {\n",
    "        'parallel_configs': [\n",
    "            experiment_config\n",
    "            for parallel_config in parallel_configs\n",
    "            for experiment_config in prepare_experiment_configs(\n",
    "                **parallel_config,\n",
    "                dataset_name=[dataset],\n",
    "                interaction_count=ic,\n",
    "            )\n",
    "        ],\n",
    "        'serial_configs': [\n",
    "            experiment_config\n",
    "            for serial_config in serial_configs\n",
    "            for experiment_config in prepare_experiment_configs(\n",
    "                **serial_config,\n",
    "                dataset_name=[dataset],\n",
    "                interaction_count=ic,\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    "    for dataset, ic in dataset_to_ic.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_KEY = 'binary-experiments'\n",
    "CONTINUE_ON_FAILURE = False\n",
    "\n",
    "if not is_cached(CACHE_KEY):\n",
    "    dfs = []\n",
    "    for dataset, configs in dataset_configs.items():\n",
    "        print(f'\\nRunning experiments for: {dataset}')\n",
    "        # Run resource-heavy ruleset_generators in serial,\n",
    "        # but allow some parallelism when not running full_ic\n",
    "        serial_rule_workers = 1 if (dataset_to_ic[dataset] == full_ic) else 2\n",
    "        dataset_serial_results = run_experiments(\n",
    "            configs['serial_configs'],\n",
    "            default_workers=2,\n",
    "            rule_workers=serial_rule_workers,\n",
    "            continue_on_failure=CONTINUE_ON_FAILURE,\n",
    "        )\n",
    "        dfs.append(summarise_experiments(dataset_serial_results, workers=8))\n",
    "        # Run lightweight ruleset_generators with more parallelism\n",
    "        dataset_parallel_results = run_experiments(\n",
    "            configs['parallel_configs'],\n",
    "            default_workers=2,\n",
    "            rule_workers=4,\n",
    "            continue_on_failure=CONTINUE_ON_FAILURE,\n",
    "        )\n",
    "        dfs.append(summarise_experiments(dataset_parallel_results, workers=8))\n",
    "    df = pd.concat(dfs)\n",
    "    save_to_cache(CACHE_KEY, df)\n",
    "\n",
    "df = load_from_cache(CACHE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = df[df['interaction_count'].isin([25, 100])].copy()\n",
    "\n",
    "# Use friendly seeded/unseeded ruleset_generator names\n",
    "rg_label_map = inverse_dict(ruleset_generators)\n",
    "\n",
    "def method_label(row):\n",
    "    if row['rule_seeder'] == BlankRS():\n",
    "        seeding = ''\n",
    "    elif row['rule_seeder'] == AccRS():\n",
    "        seeding = 'Seeded '\n",
    "    else:\n",
    "        raise ValueError('Unknown rule_seeder')\n",
    "    rg_label = rg_label_map[row['ruleset_generator']]\n",
    "    return f'{seeding}{rg_label}'\n",
    "\n",
    "table_df['method'] = table_df.apply(method_label, axis=1)\n",
    "\n",
    "# Apply different filters to results.\n",
    "full_supervision_table_df = table_df[table_df['ruleset_generator'].isin([ruleset_generators['Full supervision']])]\n",
    "table_df = table_df[\n",
    "    ~table_df['ruleset_generator'].isin([ruleset_generators['Full supervision']])\n",
    "]\n",
    "unseeded_table_df = table_df[table_df['rule_seeder'] == BlankRS()]\n",
    "seeded_table_df = table_df[table_df['rule_seeder'] == AccRS()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_label_suffix = '  '\n",
    "fig = metric_line_grid(\n",
    "    df[df['dataset_name'].isin(['imdb', 'bias_pa', 'fakenews', 'binary_agnews'])],\n",
    "    metric='test_macro_f1',\n",
    "    facet_row='dataset_name',\n",
    "    facet_col='rule_seeder',\n",
    "    ruleset_generators=ruleset_generators,\n",
    "    legend_label_suffix=legend_label_suffix,\n",
    "    category_orders={\n",
    "        'dataset_name': ['IMD', 'BPA', 'FNS', 'BAG'],\n",
    "        'rule_seeder': ['Unseeded', 'Seeded'],\n",
    "        'ruleset_generator': [rg + legend_label_suffix for rg in ruleset_generators.keys()],\n",
    "    },\n",
    ")\n",
    "fig.write_image('plots/binary-f1-lines.svg')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseeded F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseeded_f1_df = build_metric_df(pd.concat([full_supervision_table_df, unseeded_table_df]),\n",
    "                                 method='ruleset_generator',\n",
    "                                 metric='test_macro_f1',\n",
    "                                 labelled_methods=ruleset_generators)\n",
    "table = display_metric_table(unseeded_f1_df, rank_excluded_methods=['Full supervision'])\n",
    "display(table)\n",
    "print(table.to_latex(multirow_align='t', convert_css=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeded F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_f1_df = build_metric_df(seeded_table_df,\n",
    "                               method='ruleset_generator',\n",
    "                               metric='test_macro_f1',\n",
    "                               labelled_methods=ruleset_generators)\n",
    "table = display_metric_table(seeded_f1_df)\n",
    "display(table)\n",
    "print(table.to_latex(multirow_align='t', convert_css=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Friedman Test\n",
    "\n",
    "We compare the overall performance of unseeded and seeded methods across all datasets at low and high interation counts with Friedman and Nemenyi post-hoc tests presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_f1_df = build_metric_df(table_df, method='method', metric='test_macro_f1')\n",
    "display_friedman_test(full_f1_df, svg_file_prefix='plots/binary-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unseeded Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseeded_f1_std_df = build_metric_df(df[df['rule_seeder'] == BlankRS()],\n",
    "                                     method='ruleset_generator',\n",
    "                                     metric='test_macro_f1',\n",
    "                                     rngseed_agg='std',\n",
    "                                     labelled_methods=ruleset_generators)\n",
    "display(display_metric_table(unseeded_f1_std_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeded Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_f1_std_df = build_metric_df(df[df['rule_seeder'] == AccRS()],\n",
    "                                   method='ruleset_generator',\n",
    "                                   metric='test_macro_f1',\n",
    "                                   rngseed_agg='std',\n",
    "                                   labelled_methods=ruleset_generators)\n",
    "display(display_metric_table(seeded_f1_std_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_f1_std_df = pd.concat([\n",
    "    unseeded_f1_std_df,\n",
    "    seeded_f1_std_df.set_index(seeded_f1_std_df.index.map(lambda idx: (f'Seeded {idx[0]}', idx[1]))),\n",
    "])\n",
    "print('Binary classification table')\n",
    "display(median_stds_df(full_f1_std_df, datasets=dataset_to_ic.keys(), ics=min_ic))\n",
    "print('Binary classification line plot')\n",
    "display(median_stds_df(full_f1_std_df, datasets=['imdb', 'bias_pa', 'fakenews', 'binary_agnews'], ics=full_ic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
