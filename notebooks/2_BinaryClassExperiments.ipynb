{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary-class Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from witan_experiments import (is_cached,\n",
    "                               save_to_cache,\n",
    "                               load_from_cache,\n",
    "                               run_experiments)\n",
    "from witan_experiments.evaluation import (summarise_experiments,\n",
    "                                          metric_line_grid,\n",
    "                                          build_metric_df,\n",
    "                                          display_metric_table,\n",
    "                                          display_friedman_test)\n",
    "from witan_experiments.config import prepare_experiment_configs\n",
    "from witan_experiments.rule_seeders import BlankRS, AccRS\n",
    "from witan_experiments.rule_generators import (TrueRG,\n",
    "                                               IWSBinaryRG,\n",
    "                                               WitanRG,\n",
    "                                               SnubaRG,\n",
    "                                               SemiSupervisedRG,\n",
    "                                               ActiveLearningRG)\n",
    "from witan_experiments.labellers import SnorkelLblr\n",
    "from witan_experiments.models import AnnClf\n",
    "from witan_experiments.utils import inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset_generators = {\n",
    "    'Full supervision': TrueRG(),\n",
    "    'Wɪᴛᴀɴ': WitanRG(),\n",
    "    'Wɪᴛᴀɴ-Core': WitanRG(a=False, o=1),\n",
    "    'IWS-AS': IWSBinaryRG(acq='AS'),\n",
    "    'IWS-LSE-AC': IWSBinaryRG(acq='LSE'),\n",
    "    'Snuba': SnubaRG(),\n",
    "    'Semi-supervised': SemiSupervisedRG(),\n",
    "    'Active learning': ActiveLearningRG(clf=AnnClf(), init_count=0),\n",
    "}\n",
    "\n",
    "base_config = dict(\n",
    "    rule_seeder=[BlankRS()],\n",
    "    rngseed=[1],\n",
    "    ruleset_generator=list(ruleset_generators.values()),\n",
    "    labeller=[SnorkelLblr()],\n",
    "    classifier=[AnnClf()],\n",
    ")\n",
    "\n",
    "seeded_config = {\n",
    "    **base_config,\n",
    "    **dict(\n",
    "        rule_seeder=[AccRS()],\n",
    "        ruleset_generator=[\n",
    "            ruleset_generators['Wɪᴛᴀɴ'],\n",
    "            ruleset_generators['Wɪᴛᴀɴ-Core'],\n",
    "            ruleset_generators['IWS-AS'],\n",
    "            ruleset_generators['IWS-LSE-AC'],\n",
    "        ],\n",
    "    )\n",
    "}\n",
    "\n",
    "full_ic = [10, 25, 50, 100, 150, 200]\n",
    "min_ic = [25, 100]\n",
    "dataset_to_ic = {\n",
    "    'imdb': full_ic,\n",
    "    'bias_pa': full_ic,\n",
    "    'bias_pt': min_ic,\n",
    "    'bias_jp': min_ic,\n",
    "    'bias_pp': min_ic,\n",
    "    'amazon': min_ic,\n",
    "    'yelp': min_ic,\n",
    "    'plots': min_ic,\n",
    "    'fakenews': full_ic,\n",
    "    'binary_dbpedia': min_ic,\n",
    "    'binary_agnews': full_ic,\n",
    "    'airline_tweets': min_ic,\n",
    "    'damage': min_ic,\n",
    "    'spam': min_ic,\n",
    "}\n",
    "dataset_configs = {\n",
    "    dataset: [\n",
    "        *prepare_experiment_configs(**base_config, dataset_name=[dataset], interaction_count=ic),\n",
    "        *prepare_experiment_configs(**seeded_config, dataset_name=[dataset], interaction_count=ic),\n",
    "    ]\n",
    "    for dataset, ic in dataset_to_ic.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_KEY = 'binary-experiments'\n",
    "\n",
    "if not is_cached(CACHE_KEY):\n",
    "    dfs = []\n",
    "    for dataset, configs in dataset_configs.items():\n",
    "        print(f'\\nRunning experiments for: {dataset}')\n",
    "        dataset_results = run_experiments(\n",
    "            configs,\n",
    "            default_workers=2,\n",
    "            rule_workers=1,\n",
    "            continue_on_failure=False,\n",
    "        )\n",
    "        dfs.append(summarise_experiments(dataset_results, workers=8))\n",
    "    df = pd.concat(dfs)\n",
    "    save_to_cache(CACHE_KEY, df)\n",
    "\n",
    "df = load_from_cache(CACHE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = df[df['interaction_count'].isin([25, 100])].copy()\n",
    "\n",
    "# Use friendly seeded/unseeded ruleset_generator names\n",
    "rg_label_map = inverse_dict(ruleset_generators)\n",
    "\n",
    "def method_label(row):\n",
    "    if row['rule_seeder'] == BlankRS():\n",
    "        seeding = ''\n",
    "    elif row['rule_seeder'] == AccRS():\n",
    "        seeding = 'Seeded '\n",
    "    else:\n",
    "        raise ValueError('Unknown rule_seeder')\n",
    "    rg_label = rg_label_map[row['ruleset_generator']]\n",
    "    return f'{seeding}{rg_label}'\n",
    "\n",
    "table_df['method'] = table_df.apply(method_label, axis=1)\n",
    "\n",
    "# Apply different filters to results.\n",
    "full_supervision_table_df = table_df[table_df['ruleset_generator'].isin([ruleset_generators['Full supervision']])]\n",
    "table_df = table_df[\n",
    "    ~table_df['ruleset_generator'].isin([ruleset_generators['Full supervision']])\n",
    "]\n",
    "unseeded_table_df = table_df[table_df['rule_seeder'] == BlankRS()]\n",
    "seeded_table_df = table_df[table_df['rule_seeder'] == AccRS()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = metric_line_grid(\n",
    "    df[df['dataset_name'].isin(['imdb', 'bias_pa', 'fakenews', 'binary_agnews'])],\n",
    "    metric='test_macro_f1',\n",
    "    facet_row='dataset_name',\n",
    "    facet_col='rule_seeder',\n",
    "    ruleset_generators=ruleset_generators,\n",
    ")\n",
    "fig.write_image('plots/binary-f1-lines.svg')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseeded F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseeded_f1_df = build_metric_df(pd.concat([full_supervision_table_df, unseeded_table_df]),\n",
    "                                 method='ruleset_generator',\n",
    "                                 metric='test_macro_f1',\n",
    "                                 labelled_methods=ruleset_generators)\n",
    "table = display_metric_table(unseeded_f1_df, rank_excluded_methods=['Full supervision'])\n",
    "display(table)\n",
    "print(table.to_latex(multirow_align='t', convert_css=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeded F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_f1_df = build_metric_df(seeded_table_df,\n",
    "                               method='ruleset_generator',\n",
    "                               metric='test_macro_f1',\n",
    "                               labelled_methods=ruleset_generators)\n",
    "table = display_metric_table(seeded_f1_df)\n",
    "display(table)\n",
    "print(table.to_latex(multirow_align='t', convert_css=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Friedman Test\n",
    "\n",
    "We compare the overall performance of unseeded and seeded methods across all datasets at low and high interation counts with Friedman and Nemenyi post-hoc tests presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_f1_df = build_metric_df(table_df, method='method', metric='test_macro_f1')\n",
    "display_friedman_test(full_f1_df, svg_file_prefix='plots/binary-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "\n",
    "We compare the runtimes of rule generation methods after 25 and 100 user interactions. We highlight differences in runtime seconds compared to the baseline of `Wɪᴛᴀɴ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_df = build_metric_df(unseeded_table_df,\n",
    "                             method='ruleset_generator',\n",
    "                             metric='rule_gen_wall_secs',\n",
    "                             labelled_methods=ruleset_generators)\n",
    "table = display_metric_table(runtime_df, baseline_label='Wɪᴛᴀɴ', small_margin=10, big_margin=60,\n",
    "                             larger_is_better=False, formatter='{:.1f}')\n",
    "display(table)\n",
    "print(table.to_latex(multirow_align='t', convert_css=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
